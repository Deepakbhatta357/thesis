{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'c:\\Users\\ACER\\Desktop\\Chuncks-5\\chunk_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title_token_id', 'title_attention_mask', 'text_token_id',\n",
       "       'text_attention_mask'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max title token length (after truncation): 512\n",
      "Max text token length (after truncation): 512\n",
      "Invalid title attention masks (mismatch with token length) after truncation: 0\n",
      "Invalid text attention masks (mismatch with token length) after truncation: 0\n",
      "Invalid title token lengths (> 512 tokens) after truncation: 0\n",
      "Invalid text token lengths (> 512 tokens) after truncation: 0\n",
      "Empty title rows after truncation: 0\n",
      "Empty text rows after truncation: 0\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Function to ensure the attention mask is in the correct format\n",
    "def convert_to_list(mask):\n",
    "    if isinstance(mask, str):\n",
    "        return ast.literal_eval(mask)  # Convert string representation of list to an actual list\n",
    "    return mask\n",
    "\n",
    "# Convert attention masks to lists if they are in string format\n",
    "df['title_attention_mask'] = df['title_attention_mask'].apply(convert_to_list)\n",
    "df['text_attention_mask'] = df['text_attention_mask'].apply(convert_to_list)\n",
    "\n",
    "# Function to truncate tokens and attention masks to 512 tokens\n",
    "def truncate_tokens_and_masks(tokens, attention_mask, max_length=512):\n",
    "    truncated_tokens = tokens[:max_length]\n",
    "    truncated_attention_mask = attention_mask[:max_length] + [0] * (max_length - len(attention_mask))\n",
    "    return truncated_tokens, truncated_attention_mask\n",
    "\n",
    "# Apply truncation to both title and text\n",
    "df['title_token_id'], df['title_attention_mask'] = zip(*df.apply(\n",
    "    lambda row: truncate_tokens_and_masks(row['title_token_id'], row['title_attention_mask']), axis=1))\n",
    "\n",
    "df['text_token_id'], df['text_attention_mask'] = zip(*df.apply(\n",
    "    lambda row: truncate_tokens_and_masks(row['text_token_id'], row['text_attention_mask']), axis=1))\n",
    "\n",
    "# Recheck the validity after truncation\n",
    "print(\"Max title token length (after truncation):\", df['title_token_id'].apply(len).max())\n",
    "print(\"Max text token length (after truncation):\", df['text_token_id'].apply(len).max())\n",
    "\n",
    "# Recheck if attention masks match the token length\n",
    "invalid_title_attention_mask = sum(df['title_attention_mask'].apply(len) != df['title_token_id'].apply(len))\n",
    "invalid_text_attention_mask = sum(df['text_attention_mask'].apply(len) != df['text_token_id'].apply(len))\n",
    "\n",
    "print(f\"Invalid title attention masks (mismatch with token length) after truncation: {invalid_title_attention_mask}\")\n",
    "print(f\"Invalid text attention masks (mismatch with token length) after truncation: {invalid_text_attention_mask}\")\n",
    "\n",
    "# Recheck if there are any rows where the token lengths exceed 512\n",
    "invalid_title_token_length = sum(df['title_token_id'].apply(len) > 512)\n",
    "invalid_text_token_length = sum(df['text_token_id'].apply(len) > 512)\n",
    "\n",
    "print(f\"Invalid title token lengths (> 512 tokens) after truncation: {invalid_title_token_length}\")\n",
    "print(f\"Invalid text token lengths (> 512 tokens) after truncation: {invalid_text_token_length}\")\n",
    "\n",
    "# Ensure there are no empty rows\n",
    "empty_titles = df['title_token_id'].apply(len).eq(0).sum()\n",
    "empty_texts = df['text_token_id'].apply(len).eq(0).sum()\n",
    "\n",
    "print(f\"Empty title rows after truncation: {empty_titles}\")\n",
    "print(f\"Empty text rows after truncation: {empty_texts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in columns:\n",
      "title_token_id          0\n",
      "title_attention_mask    0\n",
      "text_token_id           0\n",
      "text_attention_mask     0\n",
      "dtype: int64\n",
      "\n",
      "Max token lengths:\n",
      "Max title token length: 512\n",
      "Max text token length: 512\n",
      "\n",
      "Invalid token lengths (> 512 tokens):\n",
      "Invalid title token lengths (> 512 tokens): 0\n",
      "Invalid text token lengths (> 512 tokens): 0\n",
      "\n",
      "Invalid attention masks (mismatch with token length):\n",
      "Invalid title attention masks: 0\n",
      "Invalid text attention masks: 0\n",
      "\n",
      "Empty rows:\n",
      "Empty title rows: 0\n",
      "Empty text rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if all columns are ready for embedding generation\n",
    "def final_check(df):\n",
    "    # Check for missing values\n",
    "    missing_values = df[['title_token_id', 'title_attention_mask', 'text_token_id', 'text_attention_mask']].isnull().sum()\n",
    "\n",
    "    # Check token length compliance\n",
    "    title_token_lengths = df['title_token_id'].apply(len)\n",
    "    text_token_lengths = df['text_token_id'].apply(len)\n",
    "\n",
    "    max_title_length = title_token_lengths.max()\n",
    "    max_text_length = text_token_lengths.max()\n",
    "\n",
    "    # Check if any rows have more than 512 tokens\n",
    "    invalid_title_lengths = (title_token_lengths > 512).sum()\n",
    "    invalid_text_lengths = (text_token_lengths > 512).sum()\n",
    "\n",
    "    # Check attention masks match token lengths\n",
    "    invalid_title_attention_mask = (df['title_attention_mask'].apply(len) != title_token_lengths).sum()\n",
    "    invalid_text_attention_mask = (df['text_attention_mask'].apply(len) != text_token_lengths).sum()\n",
    "\n",
    "    # Check for empty rows\n",
    "    empty_title_rows = df['title_token_id'].apply(lambda x: len(x) == 0).sum()\n",
    "    empty_text_rows = df['text_token_id'].apply(lambda x: len(x) == 0).sum()\n",
    "\n",
    "    print(\"Missing values in columns:\")\n",
    "    print(missing_values)\n",
    "    print(\"\\nMax token lengths:\")\n",
    "    print(f\"Max title token length: {max_title_length}\")\n",
    "    print(f\"Max text token length: {max_text_length}\")\n",
    "    print(\"\\nInvalid token lengths (> 512 tokens):\")\n",
    "    print(f\"Invalid title token lengths (> 512 tokens): {invalid_title_lengths}\")\n",
    "    print(f\"Invalid text token lengths (> 512 tokens): {invalid_text_lengths}\")\n",
    "    print(\"\\nInvalid attention masks (mismatch with token length):\")\n",
    "    print(f\"Invalid title attention masks: {invalid_title_attention_mask}\")\n",
    "    print(f\"Invalid text attention masks: {invalid_text_attention_mask}\")\n",
    "    print(\"\\nEmpty rows:\")\n",
    "    print(f\"Empty title rows: {empty_title_rows}\")\n",
    "    print(f\"Empty text rows: {empty_text_rows}\")\n",
    "    \n",
    "# Run final check\n",
    "final_check(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_token_id</th>\n",
       "      <th>title_attention_mask</th>\n",
       "      <th>text_token_id</th>\n",
       "      <th>text_attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 889, 31222, 11714, 79632, 11267, 852, 10...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 11081, 35247, 12347, 20691, 78530, 28462...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 885, 18187, 898, 45753, 31277, 14835, 28...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 11081, 35247, 12347, 20691, 78530, 28462...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 852, 35247, 107144, 13718, 24667, 25595,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 11081, 35247, 12347, 20691, 78530, 28462...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 887, 17279, 14835, 16373, 11081, 49545, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[101, 11081, 35247, 12347, 20691, 78530, 28462...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 882, 76549, 44388, 12512, 11186, 110091,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>[101, 11081, 35247, 12347, 20691, 78530, 28462...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_token_id  \\\n",
       "0  [101, 889, 31222, 11714, 79632, 11267, 852, 10...   \n",
       "1  [101, 885, 18187, 898, 45753, 31277, 14835, 28...   \n",
       "2  [101, 852, 35247, 107144, 13718, 24667, 25595,...   \n",
       "3  [101, 887, 17279, 14835, 16373, 11081, 49545, ...   \n",
       "4  [101, 882, 76549, 44388, 12512, 11186, 110091,...   \n",
       "\n",
       "                                title_attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
       "\n",
       "                                       text_token_id  \\\n",
       "0  [101, 11081, 35247, 12347, 20691, 78530, 28462...   \n",
       "1  [101, 11081, 35247, 12347, 20691, 78530, 28462...   \n",
       "2  [101, 11081, 35247, 12347, 20691, 78530, 28462...   \n",
       "3  [101, 11081, 35247, 12347, 20691, 78530, 28462...   \n",
       "4  [101, 11081, 35247, 12347, 20691, 78530, 28462...   \n",
       "\n",
       "                                 text_attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in columns:\n",
      "title_token_id          0\n",
      "title_attention_mask    0\n",
      "text_token_id           0\n",
      "text_attention_mask     0\n",
      "dtype: int64\n",
      "\n",
      "Data types of columns:\n",
      "title_token_id          object\n",
      "title_attention_mask    object\n",
      "text_token_id           object\n",
      "text_attention_mask     object\n",
      "dtype: object\n",
      "\n",
      "Unique lengths of token_id and attention_mask columns:\n",
      "{'title_token_id': array([512]), 'title_attention_mask': array([512]), 'text_token_id': array([512]), 'text_attention_mask': array([512])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Check for missing values (NaN) in the columns\n",
    "missing_values = df[['title_token_id', 'title_attention_mask', 'text_token_id', 'text_attention_mask']].isnull().sum()\n",
    "\n",
    "# Check the data types of the columns to ensure they are correct (likely lists or arrays for token IDs and attention masks)\n",
    "data_types = df[['title_token_id', 'title_attention_mask', 'text_token_id', 'text_attention_mask']].dtypes\n",
    "\n",
    "print(\"Missing values in columns:\")\n",
    "print(missing_values)\n",
    "\n",
    "print(\"\\nData types of columns:\")\n",
    "print(data_types)\n",
    "\n",
    "# Optionally, check if all lists or arrays have the same length (if these are lists or arrays of token ids)\n",
    "column_lengths = {\n",
    "    'title_token_id': df['title_token_id'].apply(len).unique(),\n",
    "    'title_attention_mask': df['title_attention_mask'].apply(len).unique(),\n",
    "    'text_token_id': df['text_token_id'].apply(len).unique(),\n",
    "    'text_attention_mask': df['text_attention_mask'].apply(len).unique()\n",
    "}\n",
    "\n",
    "print(\"\\nUnique lengths of token_id and attention_mask columns:\")\n",
    "print(column_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Types of elements in the columns:\n",
      "{'title_token_id': <class 'str'>, 'title_attention_mask': <class 'list'>, 'text_token_id': <class 'str'>, 'text_attention_mask': <class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "# Check the type of elements inside the columns\n",
    "element_types = {\n",
    "    'title_token_id': type(df['title_token_id'][0]),\n",
    "    'title_attention_mask': type(df['title_attention_mask'][0]),\n",
    "    'text_token_id': type(df['text_token_id'][0]),\n",
    "    'text_attention_mask': type(df['text_attention_mask'][0])\n",
    "}\n",
    "\n",
    "print(\"\\nTypes of elements in the columns:\")\n",
    "print(element_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Types of elements after conversion:\n",
      "title_token_id    object\n",
      "text_token_id     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to ensure each string is closed properly\n",
    "def fix_incomplete_token_string(token_str):\n",
    "    if token_str.endswith(\",\"):  # If the string ends with a comma but lacks the closing bracket\n",
    "        token_str = token_str[:-1]  # Remove the last comma\n",
    "    if not token_str.endswith(\"]\"):  # Check if the string does not end with a closing bracket\n",
    "        token_str = token_str + \"]\"  # Add the closing bracket\n",
    "    return token_str\n",
    "\n",
    "# Apply this fix to both 'title_token_id' and 'text_token_id'\n",
    "df['title_token_id'] = df['title_token_id'].apply(fix_incomplete_token_string)\n",
    "df['text_token_id'] = df['text_token_id'].apply(fix_incomplete_token_string)\n",
    "\n",
    "# Now safely convert the string to lists using ast.literal_eval\n",
    "df['title_token_id'] = df['title_token_id'].apply(ast.literal_eval)\n",
    "df['text_token_id'] = df['text_token_id'].apply(ast.literal_eval)\n",
    "\n",
    "# Check the result\n",
    "print(\"\\nTypes of elements after conversion:\")\n",
    "print(df[['title_token_id', 'text_token_id']].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Types of elements after conversion to lists:\n",
      "   title_token_id   text_token_id\n",
      "0  <class 'list'>  <class 'list'>\n",
      "1  <class 'list'>  <class 'list'>\n",
      "2  <class 'list'>  <class 'list'>\n",
      "3  <class 'list'>  <class 'list'>\n",
      "4  <class 'list'>  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_43864\\2754565977.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(df[['title_token_id', 'text_token_id']].applymap(type).head())\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the strings are correctly converted to lists\n",
    "df['title_token_id'] = df['title_token_id'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['text_token_id'] = df['text_token_id'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Check if they are now actual lists\n",
    "print(\"\\nTypes of elements after conversion to lists:\")\n",
    "print(df[['title_token_id', 'text_token_id']].applymap(type).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Types of elements after conversion to lists:\n",
      "title_token_id    <class 'list'>\n",
      "text_token_id     <class 'list'>\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the types of elements in the columns without using applymap\n",
    "print(\"\\nTypes of elements after conversion to lists:\")\n",
    "print(df[['title_token_id', 'text_token_id']].apply(lambda x: type(x[0])).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import ast\n",
    "\n",
    "# Function to pad or truncate sequences to length 512\n",
    "def pad_or_truncate(tokens, max_length=512):\n",
    "    return tokens[:max_length] + [0] * (max_length - len(tokens)) if len(tokens) < max_length else tokens[:max_length]\n",
    "\n",
    "# Function to generate embeddings for batches of tokenized sequences\n",
    "def generate_embeddings_in_batches(df, batch_size, device):\n",
    "    model_name = 'bert-base-multilingual-cased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    title_embeddings = []\n",
    "    text_embeddings = []\n",
    "\n",
    "    # Process the data in batches\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "\n",
    "        # Extract batch of tokenized data\n",
    "        title_tokens = df['title_token_id'][start_idx:end_idx].tolist()\n",
    "        text_tokens = df['text_token_id'][start_idx:end_idx].tolist()\n",
    "\n",
    "        # Ensure that all sequences are properly padded/truncated to 512 tokens\n",
    "        title_tokens_padded = [pad_or_truncate(tokens) for tokens in title_tokens]\n",
    "        text_tokens_padded = [pad_or_truncate(tokens) for tokens in text_tokens]\n",
    "\n",
    "        # Generate attention masks (1 for non-padding, 0 for padding)\n",
    "        title_attention_mask = [[1 if token != 0 else 0 for token in tokens] for tokens in title_tokens_padded]\n",
    "        text_attention_mask = [[1 if token != 0 else 0 for token in tokens] for tokens in text_tokens_padded]\n",
    "\n",
    "        # Convert to tensors\n",
    "        input_ids_title = torch.tensor(title_tokens_padded).to(device)\n",
    "        input_ids_text = torch.tensor(text_tokens_padded).to(device)\n",
    "        attention_mask_title = torch.tensor(title_attention_mask).to(device)\n",
    "        attention_mask_text = torch.tensor(text_attention_mask).to(device)\n",
    "\n",
    "        # Get the embeddings for title and text (token-level embeddings)\n",
    "        with torch.no_grad():\n",
    "            output_title = model(input_ids_title, attention_mask=attention_mask_title)[0]\n",
    "            output_text = model(input_ids_text, attention_mask=attention_mask_text)[0]\n",
    "\n",
    "            # Append the token-level embeddings (not averaged)\n",
    "            title_embeddings.append(output_title.cpu().numpy())\n",
    "            text_embeddings.append(output_text.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batch embeddings into one array\n",
    "    title_embeddings = np.concatenate(title_embeddings, axis=0)\n",
    "    text_embeddings = np.concatenate(text_embeddings, axis=0)\n",
    "\n",
    "    return title_embeddings, text_embeddings\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame containing the tokenized columns\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure tokenized columns are lists, not strings\n",
    "df['title_token_id'] = df['title_token_id'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['text_token_id'] = df['text_token_id'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Generate embeddings with batch size of 16\n",
    "title_embeddings, text_embeddings = generate_embeddings_in_batches(df, batch_size=16, device=device)\n",
    "\n",
    "# Save embeddings to numpy files\n",
    "np.save('title_embeddings.npy', title_embeddings)\n",
    "np.save('text_embeddings.npy', text_embeddings)\n",
    "\n",
    "print(\"Embeddings saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9683, 512, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.load('text_embeddings.npy')\n",
    "\n",
    "# Print the shape\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.isnan(embeddings).any())  # Check for NaN\n",
    "print(np.isinf(embeddings).any())  # Check for Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96371275]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example with two embeddings\n",
    "sim = cosine_similarity([embeddings[0, 0]], [embeddings[1, 0]])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 546, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1022, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1491, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings.reshape(-1, embeddings.shape[-1]))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title embeddings shape: (9683, 768)\n",
      "Text embeddings shape: (9683, 768)\n",
      "\n",
      "Sample of title embeddings: [[ 0.20855236  0.10874665  0.53104085 ...  0.4342361  -0.11074243\n",
      "  -0.06717227]\n",
      " [ 0.11317145  0.25548178  0.35806325 ...  0.06417514  0.00992666\n",
      "   0.3277155 ]\n",
      " [ 0.29713917  0.21178836  0.67768514 ...  0.25230223 -0.01435768\n",
      "   0.18460104]\n",
      " [ 0.4270823  -0.20197648  0.7608882  ...  0.399489   -0.08765424\n",
      "  -0.05823053]\n",
      " [ 0.34615248  0.0669473   0.4245155  ... -0.03246038 -0.2579427\n",
      "  -0.22427583]]\n",
      "\n",
      "Sample of text embeddings: [[ 0.1267626   0.00684246  0.4304107  ...  0.06968568  0.01075805\n",
      "  -0.22324109]\n",
      " [ 0.10639322 -0.04703356  0.41828835 ...  0.33613008 -0.0265827\n",
      "  -0.32796347]\n",
      " [ 0.35151932  0.07523967  0.42336535 ...  0.12713951  0.01377472\n",
      "  -0.17955732]\n",
      " [ 0.11898284 -0.17859691  0.5000466  ...  0.00448671  0.03648679\n",
      "  -0.24647889]\n",
      " [ 0.05929607 -0.12212496  0.30757695 ...  0.26474008 -0.00805373\n",
      "  -0.17712441]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the embeddings from the .npy files\n",
    "title_embeddings = np.load('title_embeddings.npy')\n",
    "text_embeddings = np.load('text_embeddings.npy')\n",
    "\n",
    "# Check the shape of the embeddings\n",
    "print(\"Title embeddings shape:\", title_embeddings.shape)\n",
    "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
    "\n",
    "# Optionally, check a sample of the embeddings\n",
    "print(\"\\nSample of title embeddings:\", title_embeddings[:5])\n",
    "print(\"\\nSample of text embeddings:\", text_embeddings[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
